{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization, cross validation and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains some illustrative examples to go along with the Regularization unit. There is a lot of code, most of which can be safely ignored. Focus on these take-home ideas\n",
    "- A model might fit training data very well while missing the underlying trend\n",
    "- Scikit-learn provides access to regression with l2 regularization via linear_model.Ridge() and l1 regression via linear_model.Lasso(). The two combined can be accessed with linear_model.ElasticNet(), which maintains some of the advantages of both.\n",
    "- We can add an l1 or l2 penalty to a LogisticRegression by adding penalty='l2' when creating the model, and passing in an optional parameter C, where C=1/$\\alpha$. For eg: model = linear_model.LogisticRegression(C=C, penalty='l1', tol=0.01) \n",
    "- An l1 penalty can help reduce the number of parameters on which a model relies - often a good way to combat overfitting.\n",
    "- Cross validation can be useful for tuning models. We see how using cross-validation can help us select parameters such as $\\alpha$, and demonstrate the use of model_selection.GridSearchCV() to try different combinations of parameters and select the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Demonstrating overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've looked at this before, but here's yet another example showing overfitting in a regression problem. This is a small, contrived example. The code here is to illustrate a point - don't feel the need to do more than glance over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# This bit of code silences some warnings generatd by the Logistic Regression code.\n",
    "# Scikit are changing the default solver - you can leave this out and specify the solver: LogisticRegression(solver='liblinear')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Add some plotting config\n",
    "plt.rcParams['figure.figsize'] = (12.0, 4.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed, for consistency\n",
    "random.seed(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 9, 10) \n",
    "y_star = 2 + 8*x + -0.42*(x**2) # The true function\n",
    "e = np.random.randn(len(x)) * 8 # Some errors/noise\n",
    "y = y_star + e # y now simulates noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting our generated data and the true, underlying trend\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, y_star, 'r', label='y_star')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to fit a linear model and a quadratic model, and see how well they score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(-1, 1) # Into the format we need for scikit models\n",
    "X = x\n",
    "lin = linear_model.LinearRegression() # Create the model\n",
    "lin.fit(X, y) # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the estimated model parameters to plot the predictions and to find the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_lin = lin.intercept_ + lin.coef_ * x # The model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean squared error = {:.2f}'.format(mean_squared_error(y, yhat_lin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x.reshape(-1, 1), y)\n",
    "ax.plot(x, y_star, 'r', label='y_star')\n",
    "ax.plot(x, yhat_lin, label='linear fit')\n",
    "\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model does a fairly good job describing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([x, x**2]) # Adding an x^2 term\n",
    "\n",
    "quad = linear_model.LinearRegression()\n",
    "quad.fit(X, y) #Training the model\n",
    "yhat_quad = quad.intercept_ + quad.coef_[0] * X[:, 0] + quad.coef_[1] * X[:, 1] # The quadratic model's predictions\n",
    "print('mean squared error = {:.2f}'.format(mean_squared_error(y, yhat_quad)))\n",
    "\n",
    "# Plotting...\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x.reshape(-1, 1), y)\n",
    "ax.plot(x, y_star, 'r', label='y_star')\n",
    "ax.plot(x, yhat_quad, label='quadratic fit')\n",
    "\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the MSE - the quadratic model does better than the linear model, as expected. It has succeeded in capturing the underlying trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High order polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(-1, 1)\n",
    "X = x\n",
    "degree = 10\n",
    "for k in range(2, degree + 1):\n",
    "    X = np.hstack([X, pow(x, k)])\n",
    "    \n",
    "poly = linear_model.LinearRegression()\n",
    "poly.fit(X, y)\n",
    "print('mean squared error = {:.2f}'.format(mean_squared_error(y, poly.predict(X))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the MSE, it appears that this model does best of all. But when we plot it, we see a slightly different story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new set of X, regularly increasing, to plot the prediction in more detail\n",
    "xn = np.linspace(0, 9, 100).reshape(-1, 1)\n",
    "X = xn\n",
    "degree = 10\n",
    "for k in range(2, degree + 1):\n",
    "    X = np.hstack([X, pow(xn, k)])\n",
    "yhat_poly = poly.predict(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x.reshape(-1, 1), y)\n",
    "ax.plot(x, y_star, 'r', label='y_star')\n",
    "ax.plot(xn, yhat_poly, label='polynomial fit')\n",
    "\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the same underlying process as before to simulate some new readings. This is equivalent to seeing new data. We'll see how each of the three models do in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new, noisy data\n",
    "x2 = np.linspace(0, 9, 30) \n",
    "y_star2 = 2 + 8*x2 + -0.42*(x2**2) # The true function\n",
    "e2 = np.random.randn(len(x2)) * 8 # Some errors/noise\n",
    "y2 = y_star2 + e2 # y now simulates noisy data\n",
    "\n",
    "# The new test data (model inputs)\n",
    "X1 = x2.reshape(-1, 1)\n",
    "X2 = np.hstack([X1, X1**2])\n",
    "X3 = X1\n",
    "for k in range(2, degree + 1):\n",
    "    X3 = np.hstack([X3, pow(X1, k)])\n",
    "\n",
    "# Testing with this new, unseen data\n",
    "print('Linear model mean squared error = {:.2f}'.format(mean_squared_error(y2, lin.predict(X1))))\n",
    "print('Quadratic model mean squared error = {:.2f}'.format(mean_squared_error(y2, quad.predict(X2))))\n",
    "print('10th order Polynomial mean squared error = {:.2f}'.format(mean_squared_error(y2, poly.predict(X3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the MSE for the high order model is now the highest of all three errors - it does not generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the polynomial function as before, but this time showing the new data.\n",
    "# The underlying trend is the same, but the model is not accurate\n",
    "xn = np.linspace(0, 9, 100).reshape(-1, 1)\n",
    "X = xn\n",
    "degree = 10\n",
    "for k in range(2, degree + 1):\n",
    "    X = np.hstack([X, pow(xn, k)])\n",
    "yhat_poly = poly.predict(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(x.reshape(-1, 1), y, label='training data')\n",
    "ax.scatter(x2.reshape(-1, 1), y2, label='unseen data')\n",
    "\n",
    "ax.plot(x, y_star, 'r', label='underlying trend')\n",
    "ax.plot(xn, yhat_poly, label='polynomial fit')\n",
    "\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-home idea: complex models with sparse training data are more prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to regularization. We'll show examples of each of the three types of regularization mentioned in the notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: l2 regularization and reducing coefficients\n",
    "\n",
    "We use scikit-learn's ridge regression to demonstrate L2 regularization. Note: This is adapted from a [scikit-learn example](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_coeffs.html) (BSD 3-Clause License)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate training data randomly, and study the effect of $\\alpha$, the strength of regularization, on the model coefficients (parameters). Note: You may see $\\lambda$ used instead of $\\alpha$ when talking about this regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the random data\n",
    "X, y, _ = datasets.make_regression(n_samples=10, n_features=10, coef=True,\n",
    "                          random_state=42, bias=3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll generate models with different alpha values, and compare the coefficient weights as alpha increases\n",
    "coefs = [] # Coefficients\n",
    "mses = [] # Errors\n",
    "\n",
    "n_alphas = 100 \n",
    "alphas = np.logspace(-4, 4, n_alphas)\n",
    "\n",
    "# For each alpha value, fit a model and store the parameters\n",
    "for alpha in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=alpha, fit_intercept=False)\n",
    "    ridge.fit(X, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    mses.append(mean_squared_error(ridge.predict(X), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting how the model coefficients and error change as alpha is increased:\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "# plt.gca().set_adjustable(\"box\")\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "ax.plot(alphas, coefs)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('coefficients')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('model coefficients vs strength of regularization')\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(alphas, mses)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('mean-squared-error')\n",
    "\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('model error vs strength of regularization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with higher alpha values, the coefficients are all driven to 0 and the error grows. The extreme case is undesirable, but as we'll see with real examples later on, reducing some weights is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-home idea: l2 regularization shrinks the model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: l1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to illustrate how introducing an l1 penalty produces sparser classifiers - that is, classifiers that have zero weights for more of the inputs, focusing on a smaller number of critical inputs. \n",
    "\n",
    "For a change, instead of linear regression, here we use logistic regression to demonstrate L1 regularization and compare it with l2 regularization. Note: This is adapted from a [scikit-learn example](http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html) (BSD 3-Clause License)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use logistic regression to classify images of two different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from one of scikit's built in datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# We'll try to distinguish between these two digits (a binary classification problem)\n",
    "d1 = 3\n",
    "d2 = 6\n",
    "\n",
    "# Using only those two digits\n",
    "X = X[np.where(np.logical_or(y == d1, y == d2))]\n",
    "y = y[np.where(np.logical_or(y == d1, y == d2))]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples from each of the two classes. Can you correctly identify all of them visually?\n",
    "\n",
    "N = X.shape[0]\n",
    "rand = [int(r) for r in np.random.rand(8) * N]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "for i in range(8):\n",
    "    ax = fig.add_subplot(2, 4, i+1)\n",
    "    ax.imshow(X[rand[i], :].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(y[rand[i]])\n",
    "    plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (y == d2).astype(np.int) # 0 for d1, 1 for d2 (as required for LogisticRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strength of regularization here is controlled by the parameter C, which acts as the inverse of $\\lambda$, i.e., lower values of C imply higher regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "           \n",
    "for i, C in enumerate([100, 1, 0.01]):\n",
    "    \n",
    "    log_l1 = linear_model.LogisticRegression(C=C, penalty='l1', tol=0.01)\n",
    "    log_l2 = linear_model.LogisticRegression(C=C, penalty='l2', tol=0.01)\n",
    "    \n",
    "    log_l1.fit(X, y)\n",
    "    log_l2.fit(X, y)\n",
    "    \n",
    "    coef_l1 = log_l1.coef_.ravel()\n",
    "    coef_l2 = log_l2.coef_.ravel()\n",
    "    \n",
    "    sparsity_l1 = np.mean(coef_l1 == 0)\n",
    "    sparsity_l2 = np.mean(coef_l2 == 0)\n",
    "    \n",
    "    print(\"C = {:.2f}\".format(C))\n",
    "    print(\"L1 score: {:.2f}\".format(log_l1.score(X, y)))\n",
    "    print(\"L2 score: {:.2f}\".format(log_l2.score(X, y)))\n",
    "    print(\"L1 sparsity: {:.2f}\".format(sparsity_l1))\n",
    "    print(\"L2 sparsity: {:.2f}\".format(sparsity_l2))\n",
    "    print('')\n",
    "    \n",
    "    l1plt = plt.subplot(2, 3, i+1)\n",
    "    l2plt = plt.subplot(2, 3, i+4)\n",
    "    if i == 0:\n",
    "        l1plt.set_title(\"l1 regularization\")\n",
    "        l2plt.set_title(\"l2 regularization\")\n",
    "    \n",
    "    l1plt.imshow(np.abs(coef_l1.reshape(8, 8)), interpolation='nearest',\n",
    "                 cmap='jet', vmin=0, vmax=1)\n",
    "    plt.axis('tight')\n",
    "    l2plt.imshow(np.abs(coef_l2.reshape(8, 8)), interpolation='nearest',\n",
    "                 cmap='jet', vmin=0, vmax=1)\n",
    "    plt.axis('tight')\n",
    "    \n",
    "    plt.text(2, -3, \"C = {:.1e}\".format(C))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both L2 and L1 regularized logistic classifiers yield high classification accuracies (~100%). But L1-penalized classifiers are far sparser than L2-penalized classifiers, i.e., they perform equally well but with fewer weights having non-zero values (as can be seen in the plot above). Further, the sparsity of the L1 models increases with the strength of regularization (towards the right of the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: l2 & l1 regularization with Elastic Net\n",
    "\n",
    "Finally we combine both types of regularization using scikit-learn's Elastic Net. Note: This is adapted from a [scikit-learn example](http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html) (BSD 3-Clause License)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate random X with a small number of samples (N) and a large number of dimensions (d). Using a random but sparse coefficient vector w, we generate y, and then try to recover w using regression models. Since y is only determined by the coefficients in w, any weights attached to other coefficients are due to noise in the data and are undesirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N, d = 100, 200\n",
    "\n",
    "X = np.random.randn(N, d)\n",
    "\n",
    "w = np.random.normal(scale=3, size=d)\n",
    "inds = np.arange(d)\n",
    "np.random.shuffle(inds)\n",
    "w[inds[10:]] = 0 # make sparse\n",
    "\n",
    "y = np.dot(X, w)\n",
    "y += 0.01 * np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.3\n",
    "n = int(ratio * N)\n",
    "\n",
    "X_train, y_train = X[:n], y[:n]\n",
    "X_test, y_test = X[n:], y[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A model with just l1 regularization using linear_model.Lasso\n",
    "lasso = linear_model.Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('L1 score: {:.2f}'.format(lasso.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the ElasticNet model that combines the two\n",
    "elastic = linear_model.ElasticNet(alpha=0.1, l1_ratio=0.7)\n",
    "elastic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Elastic net score: {:.2f}'.format(elastic.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "ax = fig.add_subplot(311)\n",
    "ax.plot(w)\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('underlying coefficients w')\n",
    "\n",
    "ax = fig.add_subplot(312)\n",
    "ax.plot(lasso.coef_)\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('lasso model (l1 regularization) coefficients')\n",
    "\n",
    "ax = fig.add_subplot(313)\n",
    "ax.plot(elastic.coef_)\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('elastic net (l1 & l2 regularization) coefficients')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso model (L1 regularization) fares better than Elastic Net (L2 & L1 regularization) in this case, as it manages to identify the sparse weights correctly. Elastic Net yields a less sparse coefficient vector in comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: predicting car mpg (miles per gallon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'auto-mpg.txt'\n",
    "cols = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "        'acceleration', 'year', 'origin', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(fn, names=cols, delim_whitespace=True, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars[cars.horsepower != '?']\n",
    "cars['horsepower'] = pd.to_numeric(cars['horsepower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen this dataset before. You may want to plot some scatter plots and explore it a bit, but for now let's start trying to predict the mpg. Before we split the data, we'll add a few extra columns with polynomial terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars['displacement-sq'] = cars['displacement'] ** 2\n",
    "cars['horsepower-sq'] = cars['horsepower'] ** 2\n",
    "cars['weight-sq'] = cars['weight'] ** 2\n",
    "cars.head() # Note our three new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcols = ['year', 'origin', 'cylinders', 'displacement', 'displacement-sq', 'horsepower', 'horsepower-sq', \n",
    "         'weight', 'weight-sq', 'acceleration', ]# The columns that will be inputs\n",
    "ycol = ['mpg'] # What we're trying to predict\n",
    "\n",
    "X = cars[Xcols]\n",
    "y = cars[ycol]\n",
    "\n",
    "test_ratio = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we can start trying different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we build a linear model with no regularization. We'll see how well later models compare to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mLR = linear_model.LinearRegression()\n",
    "mLR.fit(X_train, y_train)\n",
    "print('model R^2 on training dataset: {:.4f}'.format(mLR.score(X_train, y_train)))\n",
    "print('model R^2 on test dataset:     {:.4f}'.format(mLR.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, w in zip(Xcols, mLR.coef_.ravel()):\n",
    "    print('{: .4f}: {}'.format(w, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test R^2 is lower than training R^2, but not by a huge margin - we're likely not overfitting here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ridge regression (l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mR = linear_model.Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, w in zip(Xcols, mR.coef_.ravel()):\n",
    "    print('{: .4f}: {}'.format(w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('model R^2 on training dataset: {:.4f}'.format(mR.score(X_train, y_train)))\n",
    "print('model R^2 on test dataset:     {:.4f}'.format(mR.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularized model yields a very similar model to the plain linear model, indicating that there was not much overfitting with the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lasso model (l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mL = linear_model.Lasso(tol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mL.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, w in zip(Xcols, mL.coef_):\n",
    "    print('{: .4f}: {}'.format(w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('model R^2 on training dataset: {:.4f}'.format(mL.score(X_train, y_train)))\n",
    "print('model R^2 on test dataset:     {:.4f}'.format(mL.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L1 model, as expected, is sparser. Notice that only four inputs are assigned any weight. And yet it's performance on the test dataset matches or slightly exceeds that of the other two models - it is more generalizable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the test ratio to 0.95 (so only training with 5% of the data) the results are more dramatic. Linear regression scores 0.98 on the training data but only 0.5 on the test data - it has overfit thanks to the small sample size. Ridge (l2) regression fares slightly better, scoring 0.98 on the training data and 0.6 on the test data. And Lasso (l1) regression yields a sparse classifier that scores worse on the training data (0.93) but much better on the test data - 0.7. Try this for yourself by re-running the case study with test_ratio=0.95. Can you see how regularization can help when we're dealing with small training datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration of the value of cross-validation for model selection, we'll continue with the case study above, using cross validation to choose a parameter (in this case alpha) that gives the best performing model on our training data. We'll then score this model with the test data and see if it does perform better than the others we've looked at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use GridSearchCV to search through a set of parameters (or hyper-parameters). For each combination of parameters, k-fold cross-validation is performed to score a model with those parameters. We can specify the number of folds for k-fold CV. \n",
    "\n",
    "In this case, we'll search through a set of values for alpha with the ridge regression model from the previous section (mR). First, we create a dictionary with {'parameter': [values to try]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary\n",
    "param_grid = {'alpha': [   0.001,    0.01 ,    0.1  ,    1.   ,   10.   ,  100.   ,1000.   ]} # We could also vary other parameters by adding entried to this dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grid search 'model', which taker mR (the model from the case study above) as a parameter\n",
    "grid = model_selection.GridSearchCV(mR, param_grid, cv=5, iid=True) # cv=5 -> 5-fold cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train) # This runs through the parameters specified in param_grid, scoring the model with each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator:\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('grid R^2 on test sets:    ', grid.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best score: {:.4f}'.format(grid.best_score_))\n",
    "print('best model:', grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the alpha values tested, we see that $\\alpha$ = 10 was best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same approach may be used to vary hidden layer sizes or n_iterations in a nerual network, or the number of trees in Random Forest. And cross-validation means that the models are not selected based on the score with the test data but are instead chosen based on performance on the validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home point: GridSearchCV lets you try models with different parameters, finding the one that performs best in validation and is thus more likely to score well on test data and new data. This takes some of the guess work out of tuning model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
