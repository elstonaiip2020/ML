{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will go through some examples of using scikit-learn's neural network. First we'll run through a simple example with one of scikit-learn's built-in datasets, to introduce the syntax. Then we'll dive into a slightly harder example, and finally we'll load up a third example for you to practice on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - cancer classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is covered in more detail at https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html - the code here has been taken from that site and the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "First, loading an example dataset packaged with sckit-learn. There are a number of these standard datasets, some of which are often used as canonical examples when documenting machine learning methods. In this case, the breast cancer database contains descriptors for a large number of tumours that are split into two classes - benign or malignant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a dictionary containing the dataset, as well as some other information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print full description by running:\n",
    "# print(cancer['DESCR'])\n",
    "# 569 data points with 30 features. Each set of 30 features represents one tumour.\n",
    "cancer['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's already close to what we want for our model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer['data']\n",
    "y = cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0]) # The first sample, with 30 different features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[0]) # The corresponding class - malignant or benign coded as 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting and Scaling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the data into a training set and a test set - the second so that we can see the model's performance on previously unseen data.\n",
    "\n",
    "Neural networks are partiularly sensitive to input scaling, so we'll adjust our inputs to be values from 0 to 1 to improve performance - more information on why we do this will come in the next unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test/train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data - Remember, from this point until the model is trained, we can't make \n",
    "# any decisions, including scaling factor, based on the test data.\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) # This would be done to any new data you wanted to run the model on as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're trying to solve a classification problem, we'll use the MLPClassifier network proided by scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a network with three hidden layers, each with 30 neurons. This is a somewhat arbitrary choice - hyperparameter selection is still occasionally more art than science, although there are some good rules of thumb. For now, let's stick with this for the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our model has been created! We can now use it to make predictions on new data that we want to classify. Now, we'll score it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The score - pretty good! ~96% accuracy (will vary depending on the random weights the model started with)\n",
    "print(mlp.score(X_test, y_test))\n",
    "# The confusion matrix gives a better picture of what errors are slipping in\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2 - Now for something more technical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from https://archive.ics.uci.edu/ml/datasets/Robot+Execution+Failures\n",
    "It records the forces and torqes experienced when a robot tries a certain operation. Attempts are classed as 'normal', 'collision', 'obstruction' or 'fr_collision'. The goal is to try and predict which of these has ocured from the measured forces and tourques. As before, we'll be using scikit's MLPClassifier for this, but note how much extra work we need to do to get data ready for the modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('robot_execution_failure/lp1.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = f.readlines() # Read the file line by line into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:5] #The first 5 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains a class on one line followed by 15 lines, each with three force readings and three torque readings separated by tabs. You could observe this by opening the file in a text editor. These readings represent the forces etc measured over an interval after a given event. We'll start by using just the first line of readings after a class line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # inputs\n",
    "y = [] # true values\n",
    "classes = {'normal':0, 'collision':1, 'obstruction':2, 'fr_collision':3} # Encoding the classes as integers\n",
    "\n",
    "# Here, we iterate over the lines of the file. If a line matches one of our classes, we split the next line \n",
    "# to get the six readings and use those as our features. \n",
    "for i in range(len(lines) - 1):\n",
    "    line = lines[i].strip() # .strip() removes the line endings \\n\n",
    "    if line in classes.keys(): # If the line matches one of our classes (for eg, 'normal')\n",
    "        features = [int(x) for x in lines[i+1].strip().split('\\t')] # Split the next line to get our features\n",
    "        X.append(features)\n",
    "        y.append(classes[line]) # And record which class this set of features belongs to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our X and Y, we can proceed as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train)) # 66 training points - not much to work with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing to scale the inputs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the inputs \n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the neural network\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may get a convergence warning for the above step - this means the network hasn't yet learnt enough. It will still work, but this is generally a sign that we need more data or should let the network train for longer - adjusted by changing the max_iter parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network may not be performing very well - this is expected, as we have so little training data. It's amazing that it's able to get >50% accuracy, considering that we're training it with only 10 or so samples from each class. However, let's try training for a little longer, using max_iter = 1000 rather than the default: 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30), max_iter = 1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the confusion matrix - perhaps something stands out. Maybe normal operations are classified correctly, but the network struggles to distinguish between obstructions and collisions, or that one class is particularly tricky. It may be that this problem is not possible to solve given so little data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering - what if we add a variable for combined torque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = []\n",
    "for x in X:\n",
    "    torque = x[3] + x[4] + x[5]\n",
    "    X_new.append(x + [torque])\n",
    "print(X_new[0]) # Look at the first input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30), max_iter = 1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement! But it can still get much better. In the HLT assignment, you'll see how much difference adding some extra features will make, and will use more training data to improve the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3 - Wall Following Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is yet another classification problem. The data is in a slightly different format, and we'll load it into a pandas dataframe to show another way we might pre-process data. The data comes from https://archive.ics.uci.edu/ml/datasets/Wall-Following+Robot+Navigation+Data. An interesting feature of this data is that the task cannot be accomplisjed with a linear moded, but our multi-layer network is up to the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of distance sensor readings, pointing out in different directions around the robot. An example line from the file:\n",
    "'0.438,0.498,3.625,3.645,5.000,2.918,5.000,2.351,2.332,2.643,1.698,1.687,1.698,1.717,1.744,0.593,0.502,0.493,0.504,0.445,0.431,0.444,0.440,0.429,Slight-Right-Turn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data doesn't come with column names. Since there are 24 columns of data and then a class, we'll \n",
    "# label the columns accordingly\n",
    "column_names = [i+1 for i in range(24)]+['Class']\n",
    "data = pd.read_csv('wall_following/sensor_readings_24.data', names = column_names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the classes as before, and then get our X and y inputs from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'Move-Forward':0, 'Slight-Right-Turn':1, 'Sharp-Right-Turn':2, 'Slight-Left-Turn':3}\n",
    "data = data.replace({'Class': classes}) # There are various ways of doing this - the get_dummies method is an alternative\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data.loc[:,data.columns[:24]]) # The first 24 columns, corresponding to all the sensor readings\n",
    "y = data.loc[:, 'Class'] # The class column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: As before, train a network (with hidden_layer_sizes=(30,30,30)) and see ow well it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we remove those hidden layers, replacing them with a single perceptron? In this case, we have one linear set of weights multiplied by the input, and mapped through a sigmoid - similar to logistic regression. As you can see, this performs far worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new network with hidden_layer_sizes=(1) and compare the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, simplifying features or calculating new, composite features can help model accuracy. For example, the file wall_following/sensor_readings_4.data contains only 4 features per line - the minimum distance from any sensor in one of four directions. These have been calculated from the set of 24 readings we've been looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wall_following/sensor_readings_4.data', names = [1, 2, 3, 4, 'Class'])\n",
    "X = np.array(data.loc[:,[1, 2,3, 4]])\n",
    "y = data.loc[:, 'Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30))\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, the network performs even better, despite the fact that we've gone from 24 features to just four. This is worth bearing in mind, especially when you have some knowledge of the underlying system. Here, we know that sensors are noisy, but what we really want to do is avoid the walls - so looking at minimum distances makes sense. In other cases, it may be worth introducing a quadratic term to capture physical relationships (for example, given mass and velocity as features, finding mv^2 to capture some energy related relationship might help) or just simplifying sets of related features. There are few hard and fast rules for feature engineering - it's an area where experience and luck still play a major role in finding ones that work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
