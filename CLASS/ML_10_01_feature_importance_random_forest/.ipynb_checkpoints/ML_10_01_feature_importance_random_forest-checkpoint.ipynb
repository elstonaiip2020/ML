{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing feature importance via random decision forests\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dataset: Energy efficiency\n",
    "\n",
    "\n",
    "### 1.1 Description\n",
    "We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response is rounded to the nearest integer.\n",
    "\n",
    "\n",
    "### 1.2 Attribute Information:\n",
    "\n",
    "The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The aim is to use the eight features to predict each of the two responses. \n",
    "\n",
    "Specifically: \n",
    "1. X1\tRelative Compactness \n",
    "2. X2\tSurface Area \n",
    "3. X3\tWall Area \n",
    "4. X4\tRoof Area \n",
    "5. X5\tOverall Height \n",
    "6. X6\tOrientation \n",
    "7. X7\tGlazing Area \n",
    "8. X8\tGlazing Area Distribution \n",
    "9. y1\tHeating Load \n",
    "10. y2\tCooling Load\n",
    "\n",
    "### 1.3 Link \n",
    "https://archive.ics.uci.edu/ml/datasets/Energy+efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2 Load libraries and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Display graphics in Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load .xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('ENB2012_data.xlsx')\n",
    "\n",
    "# Show 5 rows\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Split dataset in train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new class attribute. Is y1+y2 above average? 1= yes / 0 = no\n",
    "dataset['aboveAVG'] = np.where((dataset['Y1'] + dataset['Y2'])>=(dataset['Y1'].mean() + dataset['Y2'].mean()), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of target\n",
    "y = dataset['aboveAVG'].values\n",
    "\n",
    "# Values of attributes\n",
    "dataset = dataset.drop(['Y1', 'Y2', 'aboveAVG'], axis=1)\n",
    "X = dataset.values\n",
    "\n",
    "# Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "## 4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n",
    "forest.fit(X_train, y_train) # fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 5 Assessing feature importance\n",
    "### 5.1 Create data frame with features and related importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['feature'] = dataset.columns # column's names\n",
    "result['importance'] = forest.feature_importances_ # values\n",
    "result.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Display results in bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Feature Importances')\n",
    "plt.bar(range(len(result['feature'])), result['importance'], color='blue', align='center')\n",
    "plt.xticks(range(len(result['feature'])), result['feature'], rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 6 Conclusion\n",
    "\"Feature importance\" measure based on Gini Importance or Mean Deacrease in Impurity (MDI), provided by Random Forest Classifier, allows us to  say that \"X4\", \"X2\", \"X1\" and \"X5\" features are very important to correctly label new entries. On ther otherhand, \"X6\" maybe could be removed from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
